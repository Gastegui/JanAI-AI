{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil as sh\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration of hyperparams and other variables that will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "\n",
    "# Autoencoder stuff\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "autoencoder_epochs = 10\n",
    "autoencoder_dropout = 0.1\n",
    "convolutional_kernel = 4\n",
    "convolutional_stride = 2\n",
    "convolutional_padding = 1\n",
    "max_pool_kernel = 2\n",
    "amount_of_pictures_to_show = 10\n",
    "encoder_name = \"models/encoder_V2.pth\"\n",
    "decoder_name = \"models/decoder_V2.pth\"\n",
    "autoencoder_name = \"models/autoencoder_V2.pth\"\n",
    "\n",
    "# Classificator stuff\n",
    "classifier_dropout = 0.1\n",
    "classification_batch_size = 64\n",
    "classification_learning_rate = 1e-3\n",
    "classification_epochs = 15\n",
    "encoder_name_to_load = encoder_name\n",
    "classifier_name = \"models/classifier_V2.pth\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "workers = os.cpu_count() - 2\n",
    "print(f\"Using {workers} workers for loading the datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the food-101 dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixFood101(path: str):\n",
    "    if os.path.exists(path):\n",
    "        if os.path.exists(path + \"/images/test/\"):\n",
    "            print(\"The structure is already fixed!\")\n",
    "            return\n",
    "        \n",
    "        file = open(path + \"/meta/train.txt\")\n",
    "        list = file.readlines()\n",
    "        file.close()\n",
    "\n",
    "        for item in list:\n",
    "            os.makedirs(path + \"/images/train/\"+item.split(\"/\")[0], exist_ok=True)\n",
    "            sh.move(path + \"/images/\"+item[:-1]+\".jpg\", path + \"/images/train/\"+item[:-1]+\".jpg\")\n",
    "\n",
    "        file = open(path + \"/meta/test.txt\")\n",
    "        list = file.readlines()\n",
    "        file.close()\n",
    "\n",
    "        for item in list:\n",
    "            os.makedirs(path + \"/images/test/\"+item.split(\"/\")[0], exist_ok=True)\n",
    "            sh.move(path + \"/images/\"+item[:-1]+\".jpg\", path + \"/images/test/\"+item[:-1]+\".jpg\")\n",
    "\n",
    "        for dirPath, _, _ in os.walk(path + \"/images/\"):\n",
    "            try:\n",
    "                os.rmdir(dirPath)\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        print(\"Couldn't find food-101\")\n",
    "fixFood101(\"./Datasets/food-101\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(org_path: str, train: bool):\n",
    "    arr = org_path.split(\"/\")\n",
    "    dst_start = \"/\".join(arr[:arr.index(\"Datasets\") + 1]) # Gets the path to the Dataset folder \n",
    "    dst = \"\"\n",
    "    if train:\n",
    "        dst = dst_start + \"/Cleaned_V2/train/\" + \"/\".join(org_path.split(\"/\")[-2:]) # ./Datasets/Cleaned_V2/train/class_name/file_name.extension\n",
    "    else:\n",
    "        dst = dst_start + \"/Cleaned_V2/validation/\" + \"/\".join(org_path.split(\"/\")[-2:]) # ./Datasets/Cleaned_V2/validation/class_name/file_name.extension\n",
    "\n",
    "    if not (dst.endswith(\".jpg\") or dst.endswith(\".jpeg\")):\n",
    "        dst = \".\".join(dst.split(\".\")[:-1]) + \".jpg\" # ./Datasets/Cleaned_V2/x/class_name/file_name.jpg\n",
    "\n",
    "    if os.path.exists(dst):\n",
    "        print(\"File already preprocessed\")\n",
    "        return\n",
    "\n",
    "    image = Image.open(org_path)\n",
    "    image = image.resize((img_size, img_size), Image.Resampling.HAMMING) # Hamming is a resampling filter that produces good quality outputs\n",
    "\n",
    "    if image.mode in (\"RGBA\", \"LA\"): #If the image has transparency, get rid of it\n",
    "        background = Image.new(\"RGB\", image.size, (255, 255, 255)) #Create a white image to act as the background\n",
    "        background.paste(image, mask=image.split()[3]) #Apply this background where there is transparency on the image\n",
    "        image = background\n",
    "\n",
    "    os.makedirs(\"/\".join(dst.split(\"/\")[:-1]), exist_ok=True) # \"/\".join(dst.split(\"/\")[:-1] = ./Datasets/Cleaned_V2/x/class_name/\n",
    "                                                              # exists_ok=True means that if the directory already exists, no error should be thrown or exception raised\n",
    "    image.save(dst, \"JPEG\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for dirPath, _, files in os.walk(\"./Datasets/food-101/images/train/\"):\n",
    "        if files:\n",
    "            for file in files:\n",
    "                preprocess(dirPath+\"/\"+file, True)\n",
    "\n",
    "    for dirPath, _, files in os.walk(\"./Datasets/food-101/images/test/\"):\n",
    "        if files:\n",
    "            for file in files:\n",
    "                preprocess(dirPath+\"/\"+file, False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Couldn't find an image. Problably the dataset doesn't exist or its path is wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Autoencoder\n",
    "\n",
    "Encodes the images first and then it decodes them, so it learns the most important patterns needed to recreate the image. <br>\n",
    "This second version has residual connections in the Residual Blocks, and skip connections in the forward funtcion of the ResidualAutoencoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualEncoderBlock(nn.Module): # The blocks follow a structure similar to ResNet V2\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualEncoderBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=autoencoder_dropout)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=convolutional_kernel, stride=convolutional_stride, padding=convolutional_padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=autoencoder_dropout)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.adjust = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=convolutional_stride, bias=False) if in_channels != out_channels else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        shorcut = x\n",
    "        if self.adjust:\n",
    "            shorcut = self.adjust(x)\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out =  out + shorcut\n",
    "        return out\n",
    "\n",
    "class ResidualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResidualEncoder, self).__init__()\n",
    "        self.block1 = ResidualEncoderBlock(3, 64)\n",
    "        self.pool1 = nn.MaxPool2d(max_pool_kernel)\n",
    "        self.block2 = ResidualEncoderBlock(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(max_pool_kernel)\n",
    "        self.block3 = ResidualEncoderBlock(128, 256)\n",
    "\n",
    "    def forward(self, x): # This forward is only used in the classificator. The autoencoder itself deals with calling the layers from the encoder, as it needs to add the skip connections\n",
    "        x = self.block1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.block3(x)\n",
    "        return x\n",
    "    \n",
    "class ResidualDecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualDecoderBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=autoencoder_dropout)\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=convolutional_kernel, stride=convolutional_stride, padding=convolutional_padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=autoencoder_dropout)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.adjust = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=1, stride=convolutional_stride, output_padding=1, bias=False) if in_channels != out_channels else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        if self.adjust:\n",
    "            shortcut = self.adjust(x)\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        out = out + shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResidualDecoder, self).__init__()\n",
    "        self.block1 = ResidualDecoderBlock(256, 128)\n",
    "        self.up1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.block2 = ResidualDecoderBlock(128*2, 64) # *2 as it has the concat of the skip connection\n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.block3 = ResidualDecoderBlock(64*2, 3) # same\n",
    "\n",
    "\n",
    "class ResidualAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResidualAutoencoder, self).__init__()\n",
    "        self.encoder = ResidualEncoder()\n",
    "        self.decoder = ResidualDecoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = [None, None]\n",
    "        \n",
    "        # Input dimensions: 256x256x3\n",
    "\n",
    "        # ENCODER\n",
    "\n",
    "        x = self.encoder.block1(x) # 128x128x64\n",
    "        skip_connections[0] = x\n",
    "        x = self.encoder.pool1(x) # 64x64x64\n",
    "\n",
    "        x = self.encoder.block2(x) # 32x32x128\n",
    "        skip_connections[1] = x\n",
    "        x = self.encoder.pool2(x) # 16x16x128\n",
    "\n",
    "        latent = self.encoder.block3(x) # 8x8x256\n",
    "\n",
    "        # DECODER\n",
    "\n",
    "        x = self.decoder.block1(latent) # 16x16x128\n",
    "        x = self.decoder.up1(x) # 32x32x128\n",
    "        \n",
    "        x = torch.cat([x, skip_connections[1]], dim=1) # 32x32x256 (the dimensions of the shortcut are concatenated to x, so they double)\n",
    "        x = self.decoder.block2(x) #64x64x64\n",
    "        x = self.decoder.up2(x) # 128x128x64\n",
    "\n",
    "        x = torch.cat([x, skip_connections[0]], dim=1) # 128x128x128\n",
    "        reconstructed = self.decoder.block3(x) # 256x256x3\n",
    "\n",
    "        return latent, reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "Uses the encoder from the autoencoder to \"get\" the important patterns of the image, and uses them for classifiying them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = True # Sets the complete encoder to retrain (so the complete encoder gets fine tuned)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256*8*8, 256*8) # 16384 -> 2048\n",
    "        self.bn1 = nn.BatchNorm1d(256 * 8)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=classifier_dropout)\n",
    "        self.fc2 = nn.Linear(256*8, 512) # 2048 -> 512\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=classifier_dropout)\n",
    "        self.fc3 = nn.Linear(512, 101)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        # \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Has a little less hue change that the V1\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizes the values to the [-1, 1] range (mean 0, stadndard eviation 1)\n",
    "])\n",
    "\n",
    "transform_validation = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder('./Datasets/Cleaned_V2/train', transform=transform_train)\n",
    "validation_dataset = ImageFolder('./Datasets/Cleaned_V2/validation', transform=transform_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=workers)\n",
    "dataloaders = {\"train\": train_loader, \"validation\": validation_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to display images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(original, reconstructed, epoch):\n",
    "    original = original.cpu().numpy().transpose(1, 2, 0)  # Convert the format of the image (H, W, C)\n",
    "    reconstructed = reconstructed.cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    # Denormalize the images so they can be printed ([-1, 1] range to [0, 1])\n",
    "    original = (original + 1) / 2\n",
    "    original = np.clip(original, 0, 1)\n",
    "    reconstructed = (reconstructed + 1) / 2\n",
    "    reconstructed = np.clip(reconstructed, 0, 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(reconstructed)\n",
    "    plt.title(f\"Reconstructed Image (Epoch {epoch+1})\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def get_selected_image():\n",
    "    iterator = iter(dataloaders[\"validation\"])\n",
    "    # Iterates a random amount of times the validation dataset. Each iteration is a different batch\n",
    "    for _ in range(0, random.randrange(0, len(dataloaders[\"validation\"]) - 1)):\n",
    "        next(iterator)\n",
    "    image, _ = next(iterator) # Gets the first image of a random batch\n",
    "    return image\n",
    "\n",
    "selected_image = get_selected_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, criterion, optimizer, num_epochs):\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "\n",
    "    since = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Training mode\n",
    "            else:\n",
    "                model.eval()  # Evaluating mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            total_samples = 0\n",
    "            \n",
    "            with tqdm(total=len(dataloaders[phase]), desc=f\"{phase} phase\") as pbar:\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        _, outputs = model(inputs)\n",
    "                        loss = criterion(outputs, inputs)\n",
    "\n",
    "                        # Backward pass and optimization only in trainig\n",
    "                        if phase == 'train':\n",
    "                            optimizer.zero_grad()\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    total_samples += inputs.size(0)\n",
    "                    \n",
    "                    pbar.set_postfix({\"Loss\": f\"{running_loss / total_samples:.4f}\"})\n",
    "                    pbar.update(1)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            print(f\"{phase} Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "            if phase == \"train\":\n",
    "                train_loss.append(float(epoch_loss))\n",
    "            else:\n",
    "                validation_loss.append(float(epoch_loss))\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # First show always the same image for a continuous evaluation\n",
    "                    fixed_image = selected_image[0].unsqueeze(0).to(device)\n",
    "                    _, reconstructed_image = model(fixed_image)\n",
    "                    show_image(fixed_image[0], reconstructed_image[0], epoch)\n",
    "\n",
    "                    # Then some random images to see how other classes are doing\n",
    "                    for _ in range(0, amount_of_pictures_to_show):\n",
    "                        iterator = iter(dataloaders[\"validation\"])\n",
    "                        for _ in range(0, random.randrange(0, len(dataloaders[\"validation\"]) - 1)):\n",
    "                            next(iterator)\n",
    "                        image, _ = next(iterator)\n",
    "\n",
    "                        i = random.randrange(0, len(image))\n",
    "                        image = image[i].unsqueeze(0).to(device)\n",
    "                        _, reconstructed_image = model.forward(image)\n",
    "                        show_image(image[0], reconstructed_image[0], epoch)\n",
    "\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Epoch finished at {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "\n",
    "    # Saving the model\n",
    "    torch.save(model.encoder.state_dict(), encoder_name)\n",
    "    torch.save(model.decoder.state_dict(), decoder_name)\n",
    "    torch.save(model.state_dict(), autoencoder_name)\n",
    "\n",
    "    plt.plot(range(len(train_loss)), train_loss, label=\"Train\")\n",
    "    plt.plot(range(len(validation_loss)), validation_loss, label=\"Validation\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend(\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResidualAutoencoder()\n",
    "model = model.to(device)\n",
    "print(f\"Device for model: {next(model.parameters()).device}\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "fixed_image = selected_image[0].unsqueeze(0).to(device)\n",
    "show_image(fixed_image[0], fixed_image[0], -2)\n",
    "\n",
    "try:\n",
    "    train_autoencoder(model, criterion, optimizer, autoencoder_epochs)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted\")\n",
    "except Exception as e:\n",
    "    print(\"Other exception \" + str(e))\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_encoder = ResidualAutoencoder().encoder\n",
    "pretrained_encoder.load_state_dict(torch.load(encoder_name_to_load))\n",
    "pretrained_encoder = pretrained_encoder.to(device)\n",
    "\n",
    "model = Classifier(pretrained_encoder).to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=classification_batch_size, shuffle=True, num_workers=workers)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=classification_batch_size, shuffle=False, num_workers=workers)\n",
    "dataloaders = {\"train\": train_loader, \"validation\": validation_loader}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    since = time.time()\n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    validation_acc = []\n",
    "    validation_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            with tqdm(total=len(dataloaders[phase]), desc=f\"{phase} phase\") as pbar:\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            # zero the parameter gradients\n",
    "                            optimizer.zero_grad()\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                    batch_acc = (torch.sum(preds == labels.data).item() / inputs.size(0)) * 100\n",
    "                    pbar.set_postfix({\"Loss\": running_loss, \"Accuracy\": f\"{batch_acc:.2f}%\"})\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step(epoch_loss)\n",
    "                \n",
    "            if phase == \"train\":\n",
    "                train_acc.append(float(epoch_acc))\n",
    "                train_loss.append(float(epoch_loss))\n",
    "            else:\n",
    "                validation_acc.append(float(epoch_acc))\n",
    "                validation_loss.append(float(epoch_loss))\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Epoch finished at {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        \n",
    "\n",
    "    torch.save(model.state_dict(), classifier_name)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(len(train_acc)), train_acc, label=\"Train\")\n",
    "    plt.plot(range(len(validation_acc)), validation_acc, label=\"Validation\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title(\"Accuracy\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(len(train_loss)), train_loss, label=\"Train\")\n",
    "    plt.plot(range(len(validation_loss)), validation_loss, label=\"Validation\")\n",
    "    plt.legend(\"lower right\")\n",
    "    plt.title(\"Loss\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=classification_learning_rate, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "try:\n",
    "    model = train_classifier(model, criterion, optimizer, scheduler, classification_epochs)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted\")\n",
    "except Exception as e:\n",
    "    print(\"Other exception: \" + str(e))\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image predictor class is in the Flask app"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
