{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil as sh\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration of hyperparams and other variables that will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "\n",
    "# Autoencoder stuff\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "dropout = 0.3\n",
    "convolutional_kernel = 4\n",
    "convolutional_stride = 2\n",
    "convolutional_padding = 1\n",
    "amount_of_pictures_to_show = 10\n",
    "encoder_name = \"models/encoder_V1.pth\"\n",
    "decoder_name = \"models/decoder_V1.pth\"\n",
    "autoencoder_name = \"models/autoencoder_V1.pth\"\n",
    "\n",
    "# Classificator stuff\n",
    "classification_batch_size = 128\n",
    "classification_learning_rate = 1e-3\n",
    "classification_epochs = 20\n",
    "encoder_name_to_load = encoder_name\n",
    "classifier_name = \"models/classifier_V1.pth\"\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "workers = os.cpu_count() - 2\n",
    "print(f\"Using {workers} workers for loading the datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the food-101 dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixFood101(path: str):\n",
    "    if os.path.exists(path):\n",
    "        if os.path.exists(path + \"/images/test/\"):\n",
    "            print(\"The structure is already fixed!\")\n",
    "            return\n",
    "        \n",
    "        file = open(path + \"/meta/train.txt\")\n",
    "        list = file.readlines()\n",
    "        file.close()\n",
    "\n",
    "        for item in list:\n",
    "            os.makedirs(path + \"/images/train/\"+item.split(\"/\")[0], exist_ok=True)\n",
    "            sh.move(path + \"/images/\"+item[:-1]+\".jpg\", path + \"/images/train/\"+item[:-1]+\".jpg\")\n",
    "\n",
    "        file = open(path + \"/meta/test.txt\")\n",
    "        list = file.readlines()\n",
    "        file.close()\n",
    "\n",
    "        for item in list:\n",
    "            os.makedirs(path + \"/images/test/\"+item.split(\"/\")[0], exist_ok=True)\n",
    "            sh.move(path + \"/images/\"+item[:-1]+\".jpg\", path + \"/images/test/\"+item[:-1]+\".jpg\")\n",
    "\n",
    "        for dirPath, _, _ in os.walk(path + \"/images/\"):\n",
    "            try:\n",
    "                os.rmdir(dirPath)\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        print(\"Couldn't find food-101\")\n",
    "fixFood101(\"./Datasets/food-101\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(org_path: str, train: bool):\n",
    "    arr = org_path.split(\"/\")\n",
    "    dst_start = \"/\".join(arr[:arr.index(\"Datasets\") + 1]) # Gets the path to the Dataset folder \n",
    "    dst = \"\"\n",
    "    if train:\n",
    "        dst = dst_start + \"/Cleaned_V1/train/\" + \"/\".join(org_path.split(\"/\")[-2:]) # ./Datasets/Cleaned_V1/train/class_name/file_name.extension\n",
    "    else:\n",
    "        dst = dst_start + \"/Cleaned_V1/validation/\" + \"/\".join(org_path.split(\"/\")[-2:]) # ./Datasets/Cleaned_V1/validation/class_name/file_name.extension\n",
    "\n",
    "    if not (dst.endswith(\".jpg\") or dst.endswith(\".jpeg\")):\n",
    "        dst = \".\".join(dst.split(\".\")[:-1]) + \".jpg\" # ./Datasets/Cleaned_V1/x/class_name/file_name.jpg\n",
    "\n",
    "    if os.path.exists(dst):\n",
    "        print(\"File already preprocessed\")\n",
    "        return\n",
    "\n",
    "    image = Image.open(org_path)\n",
    "    image = image.resize((img_size, img_size), Image.Resampling.HAMMING) # Hamming is a resampling filter that produces good quality outputs\n",
    "\n",
    "    if image.mode in (\"RGBA\", \"LA\"): #If the image has transparency, get rid of it\n",
    "        background = Image.new(\"RGB\", image.size, (255, 255, 255)) #Create a white image to act as the background\n",
    "        background.paste(image, mask=image.split()[3]) #Apply this background where there is transparency on the image\n",
    "        image = background\n",
    "\n",
    "    os.makedirs(\"/\".join(dst.split(\"/\")[:-1]), exist_ok=True) # \"/\".join(dst.split(\"/\")[:-1] = ./Datasets/Cleaned_V1/x/class_name/\n",
    "                                                              # exists_ok=True means that if the directory already exists, no error should be thrown or exception raised\n",
    "    image.save(dst, \"JPEG\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for dirPath, _, files in os.walk(\"./Datasets/food-101/images/train/\"):\n",
    "        if files:\n",
    "            for file in files:\n",
    "                preprocess(dirPath+\"/\"+file, True)\n",
    "\n",
    "    for dirPath, _, files in os.walk(\"./Datasets/food-101/images/test/\"):\n",
    "        if files:\n",
    "            for file in files:\n",
    "                preprocess(dirPath+\"/\"+file, False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Couldn't find an image. Problably the dataset doesn't exist or its path is wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "Encodes the images first and then it decodes them, so it learns the most important patterns needed to recreate the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=convolutional_kernel, stride=convolutional_stride, padding=convolutional_padding),\n",
    "            nn.LazyBatchNorm2d(), # LazyBatchNorm() doesn't need any parameter with the dimensions as it gets them itself\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=convolutional_kernel, stride=convolutional_stride, padding=convolutional_padding),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=convolutional_kernel, stride=convolutional_stride, padding=convolutional_padding),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=convolutional_kernel, stride=convolutional_stride, padding=convolutional_padding),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=convolutional_kernel, stride=convolutional_stride, padding=convolutional_padding),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=convolutional_kernel, stride=convolutional_stride, padding=convolutional_padding),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=convolutional_kernel, stride=convolutional_stride, padding=convolutional_padding),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=dropout),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=convolutional_kernel, stride=convolutional_stride, padding=convolutional_padding),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "Uses the encoder from the autoencoder to \"get\" the important patterns of the image, and uses them for classifiying them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False # Freezes all layers of the encoder so they don't get retrained (fine tuned)\n",
    "\n",
    "        # Then some of the last layers get unfrozen so they get fine tuned\n",
    "        for param in self.encoder[8].parameters():\n",
    "            param.requires_grad = True # Secon to last Convolution Normalization\n",
    "\n",
    "        for param in self.encoder[9].parameters():\n",
    "            param.requires_grad = True # Second to last Batch Normalization\n",
    "\n",
    "        for param in self.encoder[12].parameters():\n",
    "            param.requires_grad = True # Last Convolution layer\n",
    "\n",
    "        for param in self.encoder[13].parameters():\n",
    "            param.requires_grad = True # Last Batch Normalization\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 14 * 14, 101)\n",
    "            # 512*14*14 is the size of the tensor that nn.Flatten gives, and 101 the amount of classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizes the values to the [-1, 1] range (mean 0, stadndard eviation 1)\n",
    "])\n",
    "\n",
    "transform_validation = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder('./Datasets/Cleaned_V1/train', transform=transform_train)\n",
    "validation_dataset = ImageFolder('./Datasets/Cleaned_V1/validation', transform=transform_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "test_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=workers)\n",
    "dataloaders = {\"train\": train_loader, \"validation\": test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización del modelo, pérdida y optimizador\n",
    "model = Autoencoder()\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(f\"Device for model: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(original, reconstructed, epoch):\n",
    "    original = original.cpu().numpy().transpose(1, 2, 0)  # Convert the format of the image (H, W, C)\n",
    "    reconstructed = reconstructed.cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    # Denormalize the images so they can be printed( [-1, 1] range to [0, 1])\n",
    "    original = (original + 1) / 2\n",
    "    original = np.clip(original, 0, 1)\n",
    "    reconstructed = (reconstructed + 1) / 2\n",
    "    reconstructed = np.clip(reconstructed, 0, 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(reconstructed)\n",
    "    plt.title(f\"Reconstructed Image (Epoch {epoch+1})\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def get_selected_image():\n",
    "    iterator = iter(dataloaders[\"validation\"])\n",
    "    # Iterates a random amount of times the validation dataset. Each iteration is a different batch\n",
    "    for _ in range(0, random.randrange(0, len(dataloaders[\"validation\"]) - 1)):\n",
    "        next(iterator)\n",
    "    image, _ = next(iterator) # Gets the first image of a random batch\n",
    "    return image\n",
    "\n",
    "selected_image = get_selected_image() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_image = selected_image[0].unsqueeze(0).to(device)\n",
    "show_image(fixed_image[0], fixed_image[0], -2)\n",
    "\n",
    "since = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for phase in ['train', 'validation']:\n",
    "        if phase == 'train':\n",
    "            model.train()  # Training mode\n",
    "        else:\n",
    "            model.eval()  # Evaluating mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with tqdm(total=len(dataloaders[phase]), desc=f\"{phase} phase\") as pbar:\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    _, outputs = model(inputs)\n",
    "                    loss = criterion(outputs, inputs)\n",
    "\n",
    "                    # Backward pass and optimization only in training phase\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                total_samples += inputs.size(0)\n",
    "                \n",
    "                pbar.set_postfix({\"Loss\": f\"{running_loss / total_samples:.4f}\"})\n",
    "                pbar.update(1)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        print(f\"{phase} Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if phase == \"validation\":\n",
    "        # Show images passed through the autoencoder for visual evaluation\n",
    "        with torch.no_grad():\n",
    "            # First show always the same image for a continuous evaluation\n",
    "            fixed_image = selected_image[0].unsqueeze(0).to(device)\n",
    "            _, reconstructed_image = model(fixed_image)\n",
    "            show_image(fixed_image[0], reconstructed_image[0], epoch)\n",
    "\n",
    "            # Then some random images to see how other classes are doing\n",
    "            for _ in range(0, amount_of_pictures_to_show):\n",
    "                iterator = iter(dataloaders[\"validation\"])\n",
    "                for _ in range(0, random.randrange(0, len(dataloaders[\"validation\"]) - 1)):\n",
    "                    next(iterator)\n",
    "                image, _ = next(iterator)\n",
    "\n",
    "                i = random.randrange(0, len(image))\n",
    "                image = image[i].unsqueeze(0).to(device)\n",
    "                _, reconstructed_image = model.forward(image)\n",
    "                show_image(image[0], reconstructed_image[0], epoch)\n",
    "            \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Epoch finished at {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "\n",
    "# Saving the model\n",
    "torch.save(model.encoder.state_dict(), encoder_name)\n",
    "torch.save(model.decoder.state_dict(), decoder_name)\n",
    "torch.save(model.state_dict(), autoencoder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_encoder = Autoencoder().encoder\n",
    "pretrained_encoder.load_state_dict(torch.load(encoder_name_to_load))\n",
    "pretrained_encoder = pretrained_encoder.to(device)\n",
    "\n",
    "model = Classifier(pretrained_encoder).to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=classification_batch_size, shuffle=True, num_workers=workers)\n",
    "test_loader = DataLoader(validation_dataset, batch_size=classification_batch_size, shuffle=False, num_workers=workers)\n",
    "dataloaders = {\"train\": train_loader, \"validation\": test_loader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    since = time.time()\n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    validation_acc = []\n",
    "    validation_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            with tqdm(total=len(dataloaders[phase]), desc=f\"{phase} phase\") as pbar:\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            # zero the parameter gradients\n",
    "                            optimizer.zero_grad()\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                    batch_acc = (torch.sum(preds == labels.data).item() / inputs.size(0)) * 100\n",
    "                    pbar.set_postfix({\"Loss\": running_loss, \"Accuracy\": f\"{batch_acc:.2f}%\"})\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            if phase == \"train\":\n",
    "                train_acc.append(float(epoch_acc))\n",
    "                train_loss.append(float(epoch_loss))\n",
    "            else:\n",
    "                validation_acc.append(float(epoch_acc))\n",
    "                validation_loss.append(float(epoch_loss))\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Epoch finished at {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "\n",
    "    torch.save(model.state_dict(), classifier_name)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(len(train_acc)), train_acc, label=\"Train\")\n",
    "    plt.plot(range(len(validation_acc)), validation_acc, label=\"Validation\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title(\"Accuracy\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(len(train_loss)), train_loss, label=\"Train\")\n",
    "    plt.plot(range(len(validation_loss)), validation_loss, label=\"Validation\")\n",
    "    plt.legend(\"lower right\")\n",
    "    plt.title(\"Loss\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=classification_learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, criterion, optimizer, scheduler, classification_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image predictor class is in the Flask app"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
